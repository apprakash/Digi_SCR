{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q requests beautifulsoup4 selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Set up the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# URL of the website you want to interact with\n",
    "url = 'https://digiscr.sci.gov.in/'\n",
    "\n",
    "# Navigate to the webpage\n",
    "driver.get(url)\n",
    "\n",
    "cases_details = []\n",
    "\n",
    "# Wait for the dynamic content to load\n",
    "driver.implicitly_wait(60)  # Adjust the wait time according to your internet speed and website's response time\n",
    "\n",
    "# Find the <div> element with class 'content-div'\n",
    "content_div = driver.find_element(By.CLASS_NAME, 'content-div')\n",
    "\n",
    "# Option 1: Find the first <h2> within this <div>\n",
    "h2_within_div = content_div.find_element(By.TAG_NAME, 'h2')\n",
    "\n",
    "\n",
    "# Find the <ul> element(s) with the class 'linking-section'\n",
    "ul_elements = driver.find_elements(By.CLASS_NAME, 'linking-section')\n",
    "\n",
    "# Iterate through the found <ul> elements\n",
    "for ul in ul_elements:\n",
    "    li_elements = ul.find_elements(By.TAG_NAME, 'li')  # Find all <li> elements within each <ul>\n",
    "    \n",
    "    for li in li_elements:\n",
    "        # Extracting the case name and link\n",
    "        case_link_element = li.find_element(By.CSS_SELECTOR, 'a.active')\n",
    "        case_name = case_link_element.text\n",
    "        case_link = case_link_element.get_attribute('href')\n",
    "\n",
    "        # Extracting the citation\n",
    "        citation_element = li.find_element(By.CSS_SELECTOR, '.cititaion span')\n",
    "        citation = citation_element.text if citation_element else \"No citation available\"\n",
    "\n",
    "        # Extracting the date and case type\n",
    "        date_and_case_type_elements = li.find_elements(By.CSS_SELECTOR, '.civil p')\n",
    "        case_type = date_and_case_type_elements[0].text if len(date_and_case_type_elements) > 0 else \"No case type available\"\n",
    "        date = date_and_case_type_elements[1].text if len(date_and_case_type_elements) > 1 else \"No date available\"\n",
    "\n",
    "        # Extracting judge names\n",
    "        judge_names_elements = li.find_elements(By.CSS_SELECTOR, '.entryjudgment span')\n",
    "        judge_names = ', '.join([judge.text for judge in judge_names_elements])\n",
    "\n",
    "        # Extracting PDF link\n",
    "        pdf_link_element = li.find_element(By.CSS_SELECTOR, 'a[href*=\"pdf_viewer\"]')\n",
    "        pdf_link = pdf_link_element.get_attribute('href') if pdf_link_element else \"No PDF link available\"\n",
    "\n",
    "        # Print extracted information\n",
    "        cases_details.append({\n",
    "            'volume': h2_within_div.text,\n",
    "            'Case Name': case_name,\n",
    "            'Case Link': case_link,\n",
    "            'Citation': citation,\n",
    "            'Case Type': case_type,\n",
    "            'Date': date,\n",
    "            'Judge Names': judge_names,\n",
    "            'PDF Link': pdf_link\n",
    "        })\n",
    "\n",
    "for case in cases_details:\n",
    "    print(case)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "if cases_details:\n",
    "    volume_name = cases_details[0]['volume'].replace(', ', '_').replace(' ', '_').replace('.', '')\n",
    "    file_name = f\"{volume_name}.json\"\n",
    "\n",
    "    # Write the list of cases to a JSON file\n",
    "    with open(file_name, 'w') as json_file:\n",
    "        json.dump(cases_details, json_file, indent=4)\n",
    "\n",
    "    print(f\"Data saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "# Set up the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = 'https://digiscr.sci.gov.in/'\n",
    "\n",
    "# Open the URL\n",
    "driver.get(url)\n",
    "\n",
    "cases_details = []\n",
    "\n",
    "# Wait for the dynamic content to load, specifically the dropdown with id 'partno'\n",
    "wait = WebDriverWait(driver, 30)  # Wait up to 30 seconds\n",
    "partno_dropdown = wait.until(EC.presence_of_element_located((By.ID, \"partno\")))\n",
    "\n",
    "# Ensure the dropdown is in a clickable state\n",
    "ActionChains(driver).move_to_element(partno_dropdown).click(partno_dropdown).perform()\n",
    "\n",
    "# Select 'Part II' from the dropdown\n",
    "select = Select(partno_dropdown)\n",
    "select.select_by_value('2')  # Using value attribute to avoid text discrepancies\n",
    "\n",
    "# Wait for the AJAX call to complete and the records to be updated\n",
    "wait.until(lambda driver: driver.execute_script('return jQuery.active') == 0)\n",
    "\n",
    "\n",
    "# Wait for the AJAX call to complete and the records to be loaded\n",
    "# You may need to adjust the selector based on the actual content and structure\n",
    "records_span = wait.until(\n",
    "    EC.visibility_of_element_located((By.CSS_SELECTOR, \"div.records span\"))\n",
    ")\n",
    "\n",
    "# Extract and print the text from the <span> element\n",
    "total_records_text = records_span.text\n",
    "print(total_records_text)\n",
    "\n",
    "# After selecting 'Part II', wait for the relevant data to load if needed\n",
    "# Now, locate the <ul> with class 'linking-section' and count the <li> elements\n",
    "ul_element = wait.until(\n",
    "    EC.presence_of_element_located((By.CLASS_NAME, \"linking-section\"))\n",
    ")\n",
    "li_elements = ul_element.find_elements(By.TAG_NAME, 'li')\n",
    "total_li_count = len(li_elements)\n",
    "print(f\"Total <li> elements within <ul class='linking-section'>: {total_li_count}\")\n",
    "for li in li_elements:\n",
    "        # Extracting the case name and link\n",
    "        case_link_element = li.find_element(By.CSS_SELECTOR, 'a.active')\n",
    "        case_name = case_link_element.text\n",
    "        case_link = case_link_element.get_attribute('href')\n",
    "        # Extracting the citation\n",
    "        citation_element = li.find_element(By.CSS_SELECTOR, '.cititaion span')\n",
    "        citation = citation_element.text if citation_element else \"No citation available\"\n",
    "\n",
    "        # Extracting the date and case type\n",
    "        date_and_case_type_elements = li.find_elements(By.CSS_SELECTOR, '.civil p')\n",
    "        case_type = date_and_case_type_elements[0].text if len(date_and_case_type_elements) > 0 else \"No case type available\"\n",
    "        date = date_and_case_type_elements[1].text if len(date_and_case_type_elements) > 1 else \"No date available\"\n",
    "\n",
    "        # Extracting judge names\n",
    "        judge_names_elements = li.find_elements(By.CSS_SELECTOR, '.entryjudgment span')\n",
    "        judge_names = ', '.join([judge.text for judge in judge_names_elements])\n",
    "\n",
    "        # Extracting PDF link\n",
    "        pdf_link_element = li.find_element(By.CSS_SELECTOR, 'a[href*=\"pdf_viewer\"]')\n",
    "        pdf_link = pdf_link_element.get_attribute('href') if pdf_link_element else \"No PDF link available\"\n",
    "\n",
    "         # Print extracted information\n",
    "        cases_details.append({\n",
    "            'volume': '2024, Volume 1, Part II',\n",
    "            'Case Name': case_name,\n",
    "            'Case Link': case_link,\n",
    "            'Citation': citation,\n",
    "            'Case Type': case_type,\n",
    "            'Date': date,\n",
    "            'Judge Names': judge_names,\n",
    "            'PDF Link': pdf_link\n",
    "        })\n",
    "\n",
    "# Add your scraping logic here\n",
    "        \n",
    "for case in cases_details:\n",
    "    print(case)\n",
    "\n",
    "# Close the browser when done\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "if cases_details:\n",
    "    volume_name = cases_details[0]['volume'].replace(', ', '_').replace(' ', '_').replace('.', '')\n",
    "    file_name = f\"{volume_name}.json\"\n",
    "\n",
    "    # Write the list of cases to a JSON file\n",
    "    with open(file_name, 'w') as json_file:\n",
    "        json.dump(cases_details, json_file, indent=4)\n",
    "\n",
    "    print(f\"Data saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
